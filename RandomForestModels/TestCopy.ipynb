{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, sem, t\n",
    "import matplotlib.mlab as mlab\n",
    "import re\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RunRandomForest(df, question, features, feature_subset):\n",
    "    \n",
    "    df['is_train'] = np.random.uniform(0, 1, len(df)) <= .5\n",
    "    \n",
    "    for ID in feature_subset.id:\n",
    "    \n",
    "        new_header = ID + 'feature'\n",
    "        df[new_header] = pd.factorize(df[ID])[0]\n",
    "\n",
    "    # Create two new dataframes, one with the training rows, one with the test rows\n",
    "    train, test = df[df['is_train']==True], df[df['is_train']==False]\n",
    "\n",
    "    # train[Question] contains the actual species names. Before we can use it,\n",
    "    # we need to convert each species name into a digit. So, in this case there\n",
    "    # are three species, which have been coded as 0, 1, or 2.\n",
    "    y = pd.factorize(train[question])[0]\n",
    "\n",
    "    # Create a random forest classifier. By convention, clf means 'classifier'\n",
    "    clf = RandomForestClassifier(n_jobs=4,n_estimators=128);\n",
    "\n",
    "    # Train the classifier to take the training features and learn how they relate\n",
    "    # to the training y (the species)\n",
    "    clf.fit(train[features], y);\n",
    "\n",
    "    # Apply the classifier we trained to the test data (which, remember, it has never seen before)\n",
    "    clf.predict(test[features]);\n",
    "\n",
    "    # View a list of the features and their importance scores\n",
    "    return train[features], clf.feature_importances_\n",
    "\n",
    "def RepeatedRandomForests(df, question, filename, feature_subset, iterations=100):\n",
    "    \n",
    "    locs = ((df[question] != 'Valid Skip') | (df[question] != 'NA')) \n",
    "    \n",
    "    # Create a list of the feature column's names\n",
    "    features = df[locs].columns[189:]\n",
    "\n",
    "    importance_measures = np.zeros(len(features))\n",
    "\n",
    "    start = time.clock();\n",
    "\n",
    "    for i in np.arange(0,iterations):\n",
    "        \n",
    "        trained, importance = RunRandomForest(df, question, features, feature_subset);\n",
    "    \n",
    "        importance_measures = np.vstack((importance_measures, importance))\n",
    "\n",
    "    time_elapsed = time.clock() - start;\n",
    "\n",
    "    print(time_elapsed, 's')\n",
    "\n",
    "    fd = open(filename,'ab')\n",
    "    np.savetxt(fd, importance_measures, delimiter=',')\n",
    "    fd.close()\n",
    "\n",
    "    return trained, importance_measures, features\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    \n",
    "    a = 1.0*np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), sem(a)\n",
    "    h = se * t._ppf((1+confidence)/2., n-1)\n",
    "    return m, h\n",
    "\n",
    "def checkFeature(factor):\n",
    "    \n",
    "    feature = rel_importance[factor]\n",
    "    weights = np.ones_like(feature)/float(len(feature))\n",
    "\n",
    "    fig = plt.figure(num=None, figsize=(4, 3), dpi=120, \n",
    "               facecolor='w', edgecolor='k')\n",
    "    \n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    n, bins, patches = plt.hist(feature, 20, weights=weights,\n",
    "                                facecolor='green', alpha=0.75)\n",
    "    \n",
    "    plt.title(\"Distribution for \" + factor)\n",
    "    plt.xlabel(\"Relative Importance\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    ax.set_xlim((0,0.25))\n",
    "    \n",
    "def plotImportance(rel_importance, features, questions, question, iterations=100):\n",
    "    \n",
    "    fig = plt.figure(num=None, \n",
    "               figsize=(8, 12), \n",
    "               dpi=120, \n",
    "               facecolor='w', \n",
    "               edgecolor='k')\n",
    "    \n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    importances = rel_importance.mean();\n",
    "    mean, ci = mean_confidence_interval(rel_importance);\n",
    "    indices = np.argsort(importances);\n",
    "    \n",
    "    barlist = plt.barh(range(len(indices)), importances[indices], \n",
    "             color='g', align='center',\n",
    "             xerr = ci[indices])\n",
    "        \n",
    "    tags = []\n",
    "    \n",
    "    for i in range(len(features)):\n",
    "        tags.append(questions[questions['feature'] == features[indices][i]]['shortname'].values[0])\n",
    "    plt.yticks(range(len(indices)), tags)\n",
    "\n",
    "    #plt.yticks(range(len(indices)), features[indices])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.title('Important Features for ' + question)\n",
    "    \n",
    "    label = 'Iterations: ' + str(iterations)\n",
    "    \n",
    "    bbox_props = dict(boxstyle=\"round4,pad=0.3\", fc=\"white\", lw=1)\n",
    "    t = ax.text(0.18, 0.12, label, ha=\"center\", va=\"center\", rotation=0, size=12, bbox=bbox_props)\n",
    "    \n",
    "def plotCumuluativeImportance(rel_importance, features, questions, iterations=100):\n",
    "    \n",
    "    importances = rel_importance.mean()\n",
    "    mean, ci = mean_confidence_interval(rel_importance)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    fig = plt.figure(num=None, \n",
    "                   figsize=(10, 3), \n",
    "                   dpi=120, \n",
    "                   facecolor='w', \n",
    "                   edgecolor='k')\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "\n",
    "    cumulative = np.cumsum(importances[indices])\n",
    "    \n",
    "    tags = []\n",
    "    \n",
    "    for i in range(len(features)):\n",
    "        tags.append(questions[questions['feature'] == features[indices][i]]['shortname'].values[0])\n",
    "    \n",
    "    plt.errorbar(np.arange(0,len(cumulative)), cumulative, yerr=ci[indices])\n",
    "    plt.title('Cumulative Importance')\n",
    "    plt.ylabel('Importance (arb. units)')\n",
    "    plt.xlabel('Feature')\n",
    "    plt.xticks(range(len(cumulative)), tags, rotation=90);\n",
    "    \n",
    "    label = 'Iterations: ' + str(iterations)\n",
    "    \n",
    "    bbox_props = dict(boxstyle=\"round4,pad=0.3\", fc=\"white\", lw=1)\n",
    "    t = ax.text(2, 0.9, label, ha=\"center\", va=\"center\", rotation=0, size=12, bbox=bbox_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder='../survey_data/';\n",
    "XLS = pd.ExcelFile(folder + 'LabeledComputationData.xlsx')\n",
    "XLS2 = pd.ExcelFile(folder + 'CategorizedQuestions.xlsx')\n",
    "df = XLS.parse('LabeledComputationData')\n",
    "questions = XLS2.parse('Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Q1';\n",
    "filename = '../analyzed_data/TeachCompTest100.csv';\n",
    "iterations = 100;\n",
    "\n",
    "feature_subset = questions[(questions['scale'] == 'department') & \n",
    "          (questions['qtype'] !='open') & \n",
    "          (questions['context'] == 'background')];# | (questions['context'] == 'demographics'))];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trained, feature_importances, features = RepeatedRandomForests(df, question, filename, feature_subset, iterations);\n",
    "rel_importance = pd.read_csv(filename, names=trained);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotImportance(rel_importance, features, questions, question, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCumuluativeImportance(rel_importance, features, questions, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must pass DataFrame with boolean values only",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-848f0e203c6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_subset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myticklabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1958\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1961\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_mi_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1962\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_frame\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2033\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2034\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_bool_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2035\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Must pass DataFrame with boolean values only'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2036\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Must pass DataFrame with boolean values only"
     ]
    }
   ],
   "source": [
    "sns.heatmap(df[feature_subset].isnull(),yticklabels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id      feature       scale     context   qtype  \\\n",
      "184  Q185  Q185feature  department  background  binary   \n",
      "185  Q186  Q186feature  department  background  scaled   \n",
      "186  Q187  Q187feature  department  background  scaled   \n",
      "\n",
      "                  shortname     statement  \n",
      "184             HBCU status      HBCUfact  \n",
      "185        Institution type  ModClassfact  \n",
      "186  Highest degree offered   Phy.Degfact  \n"
     ]
    }
   ],
   "source": [
    "print(feature_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "locs = ((df[question] != 'Valid Skip') | (df[question] != 'NA')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[locs].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
